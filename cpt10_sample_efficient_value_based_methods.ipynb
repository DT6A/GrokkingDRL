{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "from itertools import cycle, count\n",
    "from textwrap import wrap\n",
    "\n",
    "import matplotlib\n",
    "import subprocess\n",
    "import os.path\n",
    "import tempfile\n",
    "import random\n",
    "import base64\n",
    "import pprint\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "import gym\n",
    "import io\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from gym import wrappers\n",
    "from subprocess import check_output\n",
    "from IPython.display import HTML\n",
    "\n",
    "from zoo.value_based_agents import *\n",
    "from zoo.exploration_strategies import *\n",
    "from zoo.utils import *\n",
    "from zoo.replay_buffers import *\n",
    "\n",
    "SEEDS = (12, 34, 56)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2Kel 00:00:00, ep 0000, ts 000016, ar 10 016.0±000.0, 100 016.0±000.0, ex 100 0.3±0.0, ev 009.0±000.0\n",
      "\u001b[2Kel 00:01:01, ep 0105, ts 005928, ar 10 127.7±075.1, 100 058.2±051.8, ex 100 0.4±0.1, ev 304.1±132.0\n",
      "\u001b[2Kel 00:02:02, ep 0144, ts 013433, ar 10 276.3±099.7, 100 120.9±099.6, ex 100 0.3±0.1, ev 319.8±095.8\n",
      "\u001b[2Kel 00:03:03, ep 0170, ts 020753, ar 10 396.9±106.6, 100 179.5±127.6, ex 100 0.2±0.1, ev 337.9±097.8\n",
      "\u001b[2Kel 00:04:03, ep 0185, ts 028096, ar 10 500.0±000.0, 100 244.2±155.9, ex 100 0.2±0.1, ev 368.4±107.4\n",
      "\u001b[2Kel 00:05:01, ep 0199, ts 034430, ar 10 433.4±138.2, 100 293.7±162.6, ex 100 0.2±0.1, ev 390.2±109.2\n",
      "--> reached_max_minutes ✕\n",
      "Training complete.\n",
      "Final evaluation score 500.00±0.00 in 259.22s training time, 333.17s wall-clock time.\n",
      "\n",
      "\u001b[2Kel 00:00:00, ep 0000, ts 000034, ar 10 034.0±000.0, 100 034.0±000.0, ex 100 0.6±0.0, ev 008.0±000.0\n",
      "\u001b[2Kel 00:01:01, ep 0125, ts 006325, ar 10 119.4±064.1, 100 057.2±046.0, ex 100 0.4±0.1, ev 217.4±090.1\n",
      "\u001b[2Kel 00:02:01, ep 0164, ts 013762, ar 10 220.4±023.6, 100 119.1±084.1, ex 100 0.3±0.1, ev 249.2±086.7\n",
      "\u001b[2Kel 00:03:01, ep 0195, ts 020949, ar 10 241.7±060.4, 100 174.9±088.9, ex 100 0.2±0.1, ev 291.2±094.4\n",
      "\u001b[2Kel 00:04:05, ep 0213, ts 028477, ar 10 489.8±030.6, 100 234.7±122.4, ex 100 0.2±0.1, ev 333.5±109.1\n",
      "\u001b[2Kel 00:05:02, ep 0226, ts 034866, ar 10 490.3±029.1, 100 283.8±137.2, ex 100 0.2±0.0, ev 354.8±116.6\n",
      "--> reached_max_minutes ✕\n",
      "Training complete.\n",
      "Final evaluation score 435.00±59.33 in 265.06s training time, 329.87s wall-clock time.\n",
      "\n",
      "\u001b[2Kel 00:00:00, ep 0000, ts 000012, ar 10 012.0±000.0, 100 012.0±000.0, ex 100 0.4±0.0, ev 009.0±000.0\n",
      "\u001b[2Kel 00:01:01, ep 0127, ts 006720, ar 10 152.4±087.5, 100 060.4±057.3, ex 100 0.4±0.1, ev 184.1±084.7\n",
      "\u001b[2Kel 00:02:02, ep 0164, ts 013806, ar 10 184.3±087.6, 100 120.2±090.8, ex 100 0.3±0.1, ev 252.5±116.9\n",
      "\u001b[2Kel 00:03:02, ep 0186, ts 020956, ar 10 422.0±155.8, 100 185.3±127.7, ex 100 0.2±0.1, ev 297.7±130.2\n",
      "\u001b[2Kel 00:04:06, ep 0203, ts 028360, ar 10 405.6±140.5, 100 245.4±154.0, ex 100 0.2±0.1, ev 349.4±136.4\n",
      "\u001b[2Kel 00:05:04, ep 0217, ts 035048, ar 10 490.6±028.2, 100 298.5±159.6, ex 100 0.2±0.1, ev 391.0±129.4\n",
      "--> reached_max_minutes ✕\n",
      "Training complete.\n",
      "Final evaluation score 500.00±0.00 in 267.94s training time, 335.60s wall-clock time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dueling_ddqn_results = []\n",
    "dueling_ddqn_agents, best_dueling_ddqn_agent_key, best_eval_score = {}, None, float('-inf')\n",
    "for seed in SEEDS:\n",
    "    environment_settings = {\n",
    "        'env_name': 'CartPole-v1',\n",
    "        'gamma': 1.00,\n",
    "        'max_minutes': 5,\n",
    "        'max_episodes': 2500,\n",
    "        'goal_mean_100_reward': 475\n",
    "    }\n",
    "    \n",
    "    # value_model_fn = lambda nS, nA: FCQ(nS, nA, hidden_dims=(512,128))\n",
    "    value_model_fn = lambda nS, nA: FCDuelingQ(nS, nA, hidden_dims=(512,128))\n",
    "    value_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n",
    "    value_optimizer_lr = 0.0005\n",
    "    max_gradient_norm = float('inf')\n",
    "\n",
    "    training_strategy_fn = lambda: EGreedyExpStrategy(init_epsilon=1.0,  \n",
    "                                                      min_epsilon=0.3, \n",
    "                                                      decay_steps=20000)\n",
    "    evaluation_strategy_fn = lambda: GreedyStrategy()\n",
    "\n",
    "    replay_buffer_fn = lambda: ReplayBuffer(m_size=50000, batch_size=64)\n",
    "    n_warmup_batches = 5\n",
    "    update_target_every_steps = 1\n",
    "    tau = 0.1\n",
    "\n",
    "    env_name, gamma, max_minutes, \\\n",
    "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
    "    agent = DuelingDDQN(replay_buffer_fn,\n",
    "                        value_model_fn,\n",
    "                        value_optimizer_fn,\n",
    "                        value_optimizer_lr,\n",
    "                        max_gradient_norm,\n",
    "                        training_strategy_fn,\n",
    "                        evaluation_strategy_fn,\n",
    "                        n_warmup_batches,\n",
    "                        update_target_every_steps,\n",
    "                        tau)\n",
    "\n",
    "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n",
    "    result, final_eval_score, training_time, wallclock_time = agent.train(\n",
    "        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
    "    dueling_ddqn_results.append(result)\n",
    "    dueling_ddqn_agents[seed] = agent\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_dueling_ddqn_agent_key = seed\n",
    "dueling_ddqn_results = np.array(dueling_ddqn_results)\n",
    "_ = BEEP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2Kel 00:00:00, ep 0000, ts 000016, ar 10 016.0±000.0, 100 016.0±000.0, ex 100 0.3±0.0, ev 009.0±000.0\n",
      "\u001b[2Kel 00:01:00, ep 0097, ts 005200, ar 10 168.2±070.0, 100 053.1±054.3, ex 100 0.4±0.1, ev 216.1±138.3\n",
      "\u001b[2Kel 00:02:02, ep 0126, ts 010084, ar 10 212.6±087.1, 100 094.3±083.8, ex 100 0.4±0.1, ev 272.8±119.0\n",
      "\u001b[2Kel 00:03:05, ep 0148, ts 014623, ar 10 221.4±038.4, 100 130.7±089.5, ex 100 0.3±0.1, ev 291.9±114.3\n",
      "\u001b[2Kel 00:04:05, ep 0168, ts 018433, ar 10 204.5±048.3, 100 160.5±081.5, ex 100 0.2±0.1, ev 316.2±107.6\n",
      "\u001b[2Kel 00:05:01, ep 0186, ts 021730, ar 10 140.3±062.7, 100 182.8±074.9, ex 100 0.2±0.1, ev 310.4±104.4\n",
      "--> reached_max_minutes ✕\n",
      "Training complete.\n",
      "Final evaluation score 248.43±75.75 in 271.82s training time, 316.98s wall-clock time.\n",
      "\n",
      "\u001b[2Kel 00:00:00, ep 0000, ts 000034, ar 10 034.0±000.0, 100 034.0±000.0, ex 100 0.6±0.0, ev 008.0±000.0\n",
      "\u001b[2Kel 00:01:00, ep 0106, ts 005027, ar 10 139.7±088.2, 100 048.8±051.3, ex 100 0.4±0.1, ev 217.5±118.6\n",
      "\u001b[2Kel 00:02:02, ep 0126, ts 009908, ar 10 276.3±081.6, 100 093.1±106.4, ex 100 0.4±0.1, ev 291.9±116.0\n",
      "\u001b[2Kel 00:03:07, ep 0141, ts 014287, ar 10 278.6±087.1, 100 132.9±129.0, ex 100 0.3±0.1, ev 314.2±118.5\n",
      "\u001b[2Kel 00:04:10, ep 0152, ts 018207, ar 10 348.6±153.5, 100 168.1±149.1, ex 100 0.3±0.1, ev 337.5±122.7\n",
      "\u001b[2Kel 00:05:01, ep 0160, ts 021170, ar 10 381.5±066.1, 100 195.5±153.3, ex 100 0.3±0.1, ev 349.0±116.8\n",
      "--> reached_max_minutes ✕\n",
      "Training complete.\n",
      "Final evaluation score 454.52±71.98 in 273.43s training time, 331.69s wall-clock time.\n",
      "\n",
      "\u001b[2Kel 00:00:00, ep 0000, ts 000012, ar 10 012.0±000.0, 100 012.0±000.0, ex 100 0.4±0.0, ev 009.0±000.0\n",
      "\u001b[2Kel 00:01:01, ep 0102, ts 005320, ar 10 067.5±059.1, 100 052.7±046.0, ex 100 0.4±0.1, ev 183.2±132.2\n",
      "\u001b[2Kel 00:02:04, ep 0123, ts 010033, ar 10 277.0±077.2, 100 094.9±096.8, ex 100 0.3±0.1, ev 254.4±122.1\n",
      "\u001b[2Kel 00:03:05, ep 0140, ts 014041, ar 10 229.1±081.0, 100 129.8±107.7, ex 100 0.3±0.1, ev 282.7±117.2\n",
      "\u001b[2Kel 00:04:11, ep 0155, ts 017866, ar 10 262.1±070.3, 100 162.2±110.6, ex 100 0.3±0.1, ev 310.7±112.7\n",
      "\u001b[2Kel 00:05:01, ep 0167, ts 020600, ar 10 219.3±042.7, 100 182.3±106.1, ex 100 0.2±0.1, ev 322.6±107.4\n",
      "--> reached_max_minutes ✕\n"
     ]
    }
   ],
   "source": [
    "per_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "for seed in SEEDS:\n",
    "    environment_settings = {\n",
    "        'env_name': 'CartPole-v1',\n",
    "        'gamma': 1.00,\n",
    "        'max_minutes': 5,\n",
    "        'max_episodes': 2500,\n",
    "        'goal_mean_100_reward': 475\n",
    "    }\n",
    "\n",
    "    value_model_fn = lambda nS, nA: FCDuelingQ(nS, nA, hidden_dims=(512,128))\n",
    "    value_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n",
    "    value_optimizer_lr = 0.0005\n",
    "    max_gradient_norm = float('inf')\n",
    "\n",
    "    training_strategy_fn = lambda: EGreedyExpStrategy(init_epsilon=1.0,  \n",
    "                                                      min_epsilon=0.3, \n",
    "                                                      decay_steps=20000)\n",
    "    evaluation_strategy_fn = lambda: GreedyStrategy()\n",
    "\n",
    "    # replay_buffer_fn = lambda: ReplayBuffer(max_size=10000, batch_size=64)\n",
    "    # replay_buffer_fn = lambda: PrioritizedReplayBuffer(\n",
    "    #     max_samples=10000, batch_size=64, rank_based=True, \n",
    "    #     alpha=0.6, beta0=0.1, beta_rate=0.99995)\n",
    "    replay_buffer_fn = lambda: PrioritizedReplayBuffer(\n",
    "        max_samples=20000, batch_size=64, rank_based=False,\n",
    "        alpha=0.6, beta0=0.1, beta_rate=0.99995)\n",
    "    n_warmup_batches = 5\n",
    "    update_target_every_steps = 1\n",
    "    tau = 0.1\n",
    "\n",
    "    env_name, gamma, max_minutes, \\\n",
    "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
    "    agent = PER(replay_buffer_fn, \n",
    "                value_model_fn, \n",
    "                value_optimizer_fn, \n",
    "                value_optimizer_lr,\n",
    "                max_gradient_norm,\n",
    "                training_strategy_fn,\n",
    "                evaluation_strategy_fn,\n",
    "                n_warmup_batches,\n",
    "                update_target_every_steps,\n",
    "                tau)\n",
    "\n",
    "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n",
    "    result, final_eval_score, training_time, wallclock_time = agent.train(\n",
    "        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
    "    per_results.append(result)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_agent = agent\n",
    "per_results = np.array(per_results)\n",
    "_ = BEEP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
