{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "cpt11_policy_gradient_and_actor_critic_methods.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-THqFGGL_dy7"
      },
      "source": [
        "!mkdir zoo; cd zoo; touch __init__.py"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMkNLTWQ_KlV"
      },
      "source": [
        "import warnings ; warnings.filterwarnings('ignore')\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "from collections import namedtuple, deque\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pylab as pylab\n",
        "from itertools import cycle, count\n",
        "from textwrap import wrap\n",
        "\n",
        "import matplotlib\n",
        "import subprocess\n",
        "import os.path\n",
        "import tempfile\n",
        "import random\n",
        "import base64\n",
        "import pprint\n",
        "import glob\n",
        "import time\n",
        "import json\n",
        "import sys\n",
        "import gym\n",
        "import io\n",
        "import os\n",
        "import gc\n",
        "\n",
        "from gym import wrappers\n",
        "from subprocess import check_output\n",
        "from IPython.display import HTML\n",
        "\n",
        "from zoo.policy_gradient_agents import *\n",
        "from zoo.actor_critic_agents import *\n",
        "from zoo.utils import *\n",
        "\n",
        "SEEDS = (12, 34, 56)\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2xRRSDh_KlW",
        "outputId": "2f759c44-53f5-44e2-bc72-dfcadcc4b08a"
      },
      "source": [
        "reinforce_results = []\n",
        "best_agent, best_eval_score = None, float('-inf')\n",
        "for seed in SEEDS:\n",
        "    environment_settings = {\n",
        "        'env_name': 'CartPole-v1',\n",
        "        'gamma': 1.00,\n",
        "        'max_minutes': 5,\n",
        "        'max_episodes': 5000,\n",
        "        'goal_mean_100_reward': 475\n",
        "    }\n",
        "\n",
        "    policy_model_fn = lambda nS, nA: FCDAP(nS, nA, hidden_dims=(128,64))\n",
        "    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
        "    policy_optimizer_lr = 0.0005\n",
        "\n",
        "    env_name, gamma, max_minutes, \\\n",
        "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
        "    agent = REINFORCE(policy_model_fn, policy_optimizer_fn, policy_optimizer_lr)\n",
        "\n",
        "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n",
        "    # make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name, unwrapped=True)\n",
        "    # make_env_fn, make_env_kargs = get_make_env_fn(\n",
        "    #     env_name=env_name, addon_wrappers=[MCCartPole,])\n",
        "    result, final_eval_score, training_time, wallclock_time = agent.train(\n",
        "        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
        "    reinforce_results.append(result)\n",
        "    if final_eval_score > best_eval_score:\n",
        "        best_eval_score = final_eval_score\n",
        "        best_agent = agent\n",
        "reinforce_results = np.array(reinforce_results)\n",
        "_ = BEEP()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[2Kel 00:00:00, ep 0000, ts 000020, ar 10 020.0±000.0, 100 020.0±000.0, ex 100 0.5±0.0, ev 012.0±000.0\n",
            "\u001b[2Kel 00:01:00, ep 0403, ts 030072, ar 10 207.1±092.1, 100 164.4±086.7, ex 100 0.3±0.0, ev 296.0±132.1\n",
            "\u001b[2Kel 00:02:00, ep 0598, ts 072569, ar 10 334.5±106.2, 100 231.2±106.6, ex 100 0.3±0.0, ev 311.4±148.6\n",
            "\u001b[2Kel 00:02:47, ep 0703, ts 107979, ar 10 373.6±129.8, 100 334.2±127.6, ex 100 0.3±0.0, ev 475.5±056.7\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 500.00±0.00 in 121.52s training time, 177.81s wall-clock time.\n",
            "\n",
            "\u001b[2Kel 00:00:00, ep 0000, ts 000019, ar 10 019.0±000.0, 100 019.0±000.0, ex 100 0.5±0.0, ev 010.0±000.0\n",
            "\u001b[2Kel 00:01:00, ep 0499, ts 025611, ar 10 106.4±040.0, 100 117.3±074.0, ex 100 0.3±0.0, ev 227.4±115.6\n",
            "\u001b[2Kel 00:02:00, ep 0690, ts 064263, ar 10 225.8±057.1, 100 195.4±092.2, ex 100 0.3±0.0, ev 306.6±137.2\n",
            "\u001b[2Kel 00:03:00, ep 0843, ts 106945, ar 10 352.7±039.5, 100 300.9±124.4, ex 100 0.3±0.0, ev 406.6±134.0\n",
            "\u001b[2Kel 00:04:01, ep 1029, ts 148419, ar 10 356.9±147.2, 100 261.8±153.2, ex 100 0.3±0.0, ev 358.1±167.7\n",
            "\u001b[2Kel 00:04:18, ep 1062, ts 161697, ar 10 362.2±136.2, 100 359.2±126.2, ex 100 0.3±0.0, ev 476.6±071.0\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 500.00±0.00 in 185.10s training time, 268.38s wall-clock time.\n",
            "\n",
            "\u001b[2Kel 00:00:00, ep 0000, ts 000016, ar 10 016.0±000.0, 100 016.0±000.0, ex 100 0.7±0.0, ev 014.0±000.0\n",
            "\u001b[2Kel 00:01:00, ep 0458, ts 023495, ar 10 185.6±091.7, 100 109.5±079.5, ex 100 0.4±0.1, ev 262.3±146.8\n",
            "\u001b[2Kel 00:02:00, ep 0710, ts 062518, ar 10 417.1±112.5, 100 236.3±190.9, ex 100 0.3±0.0, ev 293.6±202.0\n",
            "\u001b[2Kel 00:02:24, ep 0754, ts 082591, ar 10 494.7±015.9, 100 412.8±123.1, ex 100 0.3±0.0, ev 476.9±076.8\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 500.00±0.00 in 103.57s training time, 154.69s wall-clock time.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIdl6OCF_KlX",
        "outputId": "8b2015e2-1297-491b-8e89-14fca6366ec8"
      },
      "source": [
        "vpg_results = []\n",
        "best_agent, best_eval_score = None, float('-inf')\n",
        "for seed in SEEDS:\n",
        "    environment_settings = {\n",
        "        'env_name': 'CartPole-v1',\n",
        "        'gamma': 1.00,\n",
        "        'max_minutes': 10,\n",
        "        'max_episodes': 10000,\n",
        "        'goal_mean_100_reward': 475\n",
        "    }\n",
        "\n",
        "    policy_model_fn = lambda nS, nA: FCDAP(nS, nA, hidden_dims=(128,64))\n",
        "    policy_model_max_grad_norm = 1\n",
        "    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
        "    policy_optimizer_lr = 0.0005\n",
        "\n",
        "    value_model_fn = lambda nS: FCV(nS, hidden_dims=(256,128))\n",
        "    value_model_max_grad_norm = float('inf')\n",
        "    value_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n",
        "    value_optimizer_lr = 0.0007\n",
        "\n",
        "    entropy_loss_weight = 0.001\n",
        "\n",
        "    env_name, gamma, max_minutes, \\\n",
        "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
        "    agent = VPG(policy_model_fn, \n",
        "                policy_model_max_grad_norm, \n",
        "                policy_optimizer_fn, \n",
        "                policy_optimizer_lr,\n",
        "                value_model_fn, \n",
        "                value_model_max_grad_norm, \n",
        "                value_optimizer_fn, \n",
        "                value_optimizer_lr, \n",
        "                entropy_loss_weight)\n",
        "\n",
        "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n",
        "    result, final_eval_score, training_time, wallclock_time = agent.train(\n",
        "        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
        "    vpg_results.append(result)\n",
        "    if final_eval_score > best_eval_score:\n",
        "        best_eval_score = final_eval_score\n",
        "        best_agent = agent\n",
        "vpg_results = np.array(vpg_results)\n",
        "_ = BEEP()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[2Kel 00:00:00, ep 0000, ts 000015, ar 10 015.0±000.0, 100 015.0±000.0, ex 100 0.7±0.0, ev 022.0±000.0\n",
            "\u001b[2Kel 00:01:00, ep 0248, ts 019771, ar 10 159.1±062.9, 100 134.8±086.1, ex 100 0.3±0.0, ev 345.5±126.5\n",
            "\u001b[2Kel 00:02:00, ep 0359, ts 045432, ar 10 307.4±077.9, 100 238.0±114.0, ex 100 0.3±0.0, ev 405.8±114.0\n",
            "\u001b[2Kel 00:03:01, ep 0451, ts 077933, ar 10 436.1±109.9, 100 349.5±111.3, ex 100 0.3±0.0, ev 468.3±055.0\n",
            "\u001b[2Kel 00:03:32, ep 0489, ts 096264, ar 10 493.7±011.1, 100 415.0±104.3, ex 100 0.3±0.0, ev 476.1±046.5\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 499.63±3.68 in 169.36s training time, 221.81s wall-clock time.\n",
            "\n",
            "\u001b[2Kel 00:00:00, ep 0000, ts 000016, ar 10 016.0±000.0, 100 016.0±000.0, ex 100 0.5±0.0, ev 010.0±000.0\n",
            "\u001b[2Kel 00:01:00, ep 0286, ts 026565, ar 10 257.7±068.4, 100 178.5±105.3, ex 100 0.3±0.0, ev 370.0±121.3\n",
            "\u001b[2Kel 00:02:00, ep 0393, ts 056297, ar 10 435.7±073.3, 100 282.5±110.7, ex 100 0.3±0.0, ev 374.9±123.9\n",
            "\u001b[2Kel 00:03:01, ep 0477, ts 088902, ar 10 483.1±050.7, 100 393.7±110.2, ex 100 0.3±0.0, ev 466.0±077.0\n",
            "\u001b[2Kel 00:03:19, ep 0496, ts 097961, ar 10 475.9±072.3, 100 406.0±113.5, ex 100 0.2±0.0, ev 475.6±067.9\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 500.00±0.00 in 161.05s training time, 208.73s wall-clock time.\n",
            "\n",
            "\u001b[2Kel 00:00:00, ep 0000, ts 000012, ar 10 012.0±000.0, 100 012.0±000.0, ex 100 0.2±0.0, ev 009.0±000.0\n",
            "\u001b[2Kel 00:01:00, ep 0261, ts 022246, ar 10 175.6±062.9, 100 149.8±079.3, ex 100 0.3±0.0, ev 386.0±117.8\n",
            "\u001b[2Kel 00:02:00, ep 0369, ts 052393, ar 10 361.6±143.2, 100 282.9±116.5, ex 100 0.3±0.0, ev 452.5±083.0\n",
            "\u001b[2Kel 00:02:34, ep 0417, ts 070984, ar 10 459.0±082.6, 100 356.6±125.1, ex 100 0.3±0.0, ev 476.4±065.8\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 499.66±3.38 in 117.03s training time, 164.14s wall-clock time.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC-PT3Uj_KlY",
        "outputId": "36fff007-f93e-4903-9c8f-6f010c86cff7"
      },
      "source": [
        "a3c_results = []\n",
        "best_agent, best_eval_score = None, float('-inf')\n",
        "def policy_model_fn (nS, nA):\n",
        "    return FCDAP(nS, nA, hidden_dims=(128,64))\n",
        "policy_model_max_grad_norm = 1\n",
        "def policy_optimizer_fn(net, lr):\n",
        "    return SharedAdam(net.parameters(), lr=lr)\n",
        "policy_optimizer_lr = 0.0005\n",
        "\n",
        "def value_model_fn(nS):\n",
        "    return FCV(nS, hidden_dims=(256,128))\n",
        "value_model_max_grad_norm = float('inf')\n",
        "def value_optimizer_fn(net, lr):\n",
        "    return SharedRMSprop(net.parameters(), lr=lr)\n",
        "value_optimizer_lr = 0.0007\n",
        "\n",
        "def make_env_fn(env_name, seed=None, render=None, record=False,\n",
        "                unwrapped=False, monitor_mode=None,\n",
        "                inner_wrappers=None, outer_wrappers=None):\n",
        "    mdir = tempfile.mkdtemp()\n",
        "    env = None\n",
        "    if render:\n",
        "        try:\n",
        "            env = gym.make(env_name, render=render)\n",
        "        except:\n",
        "            pass\n",
        "    if env is None:\n",
        "        env = gym.make(env_name)\n",
        "    if seed is not None: env.seed(seed)\n",
        "    env = env.unwrapped if unwrapped else env\n",
        "    if inner_wrappers:\n",
        "        for wrapper in inner_wrappers:\n",
        "            env = wrapper(env)\n",
        "    env = wrappers.Monitor(\n",
        "        env, mdir, force=True,\n",
        "        mode=monitor_mode,\n",
        "        video_callable=lambda e_idx: record) if monitor_mode else env\n",
        "    if outer_wrappers:\n",
        "        for wrapper in outer_wrappers:\n",
        "            env = wrapper(env)\n",
        "    return env\n",
        "\n",
        "for seed in SEEDS:\n",
        "    environment_settings = {\n",
        "        'env_name': 'CartPole-v1',\n",
        "        'gamma': 1.00,\n",
        "        'max_minutes': 5,\n",
        "        'max_episodes': 2500,\n",
        "        'goal_mean_100_reward': 475\n",
        "    }\n",
        "    \n",
        "\n",
        "    entropy_loss_weight = 0.001\n",
        "\n",
        "    max_n_steps = 50\n",
        "    n_workers = 1\n",
        "\n",
        "    env_name, gamma, max_minutes, \\\n",
        "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
        "    agent = A3C(policy_model_fn,\n",
        "                policy_model_max_grad_norm, \n",
        "                policy_optimizer_fn, \n",
        "                policy_optimizer_lr,\n",
        "                value_model_fn,\n",
        "                value_model_max_grad_norm,\n",
        "                value_optimizer_fn, \n",
        "                value_optimizer_lr,\n",
        "                entropy_loss_weight, \n",
        "                max_n_steps,\n",
        "                n_workers)\n",
        "\n",
        "    #make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n",
        "    make_env_kargs = {'env_name':env_name}\n",
        "    result, final_eval_score, training_time, wallclock_time = agent.train(\n",
        "        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
        "    a3c_results.append(result)\n",
        "    if final_eval_score > best_eval_score:\n",
        "        best_eval_score = final_eval_score\n",
        "        best_agent = agent\n",
        "a3c_results = np.array(a3c_results)\n",
        "_ = BEEP()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[2Kel 00:00:00, ep 0000, ts 000015, ar 10 015.0±000.0, 100 015.0±000.0, ex 100 0.7±0.0, ev 022.0±000.0\n",
            "\u001b[2Kel 00:01:00, ep 0310, ts 039161, ar 10 322.8±098.8, 100 261.4±133.1, ex 100 0.3±0.0, ev 308.5±144.7\n",
            "\u001b[2Kel 00:02:00, ep 0455, ts 089022, ar 10 220.3±010.9, 100 379.2±129.2, ex 100 0.2±0.0, ev 382.6±124.8\n",
            "\u001b[2Kel 00:02:57, ep 0564, ts 138018, ar 10 500.0±000.0, 100 471.5±074.5, ex 100 0.2±0.0, ev 475.2±068.6\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 500.00±0.00 in 153.98s training time, 177.61s wall-clock time.\n",
            "\n",
            "\u001b[2Kel 00:00:00, ep 0000, ts 000016, ar 10 016.0±000.0, 100 016.0±000.0, ex 100 0.5±0.0, ev 010.0±000.0\n",
            "\u001b[2Kel 00:01:00, ep 0328, ts 036103, ar 10 199.9±033.5, 100 168.3±105.7, ex 100 0.3±0.0, ev 220.7±143.6\n",
            "\u001b[2Kel 00:02:00, ep 0500, ts 083437, ar 10 500.0±000.0, 100 365.7±138.4, ex 100 0.2±0.0, ev 422.2±116.7\n",
            "\u001b[2Kel 00:03:00, ep 0674, ts 131401, ar 10 371.1±107.4, 100 302.7±179.2, ex 100 0.2±0.0, ev 309.0±175.6\n",
            "\u001b[2Kel 00:03:26, ep 0719, ts 153901, ar 10 500.0±000.0, 100 472.6±073.5, ex 100 0.2±0.0, ev 475.4±067.8\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 500.00±0.00 in 178.57s training time, 207.02s wall-clock time.\n",
            "\n",
            "\u001b[2Kel 00:00:00, ep 0000, ts 000012, ar 10 012.0±000.0, 100 012.0±000.0, ex 100 0.2±0.0, ev 009.0±000.0\n",
            "\u001b[2Kel 00:01:00, ep 0329, ts 037167, ar 10 126.1±043.9, 100 169.4±110.1, ex 100 0.3±0.0, ev 198.3±130.1\n",
            "\u001b[2Kel 00:02:00, ep 0490, ts 087418, ar 10 481.8±057.6, 100 412.0±138.4, ex 100 0.2±0.0, ev 429.2±128.6\n",
            "\u001b[2Kel 00:02:22, ep 0530, ts 105293, ar 10 500.0±000.0, 100 475.2±071.5, ex 100 0.2±0.0, ev 475.9±069.0\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 500.00±0.00 in 121.96s training time, 142.79s wall-clock time.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjQZFzNU_KlY",
        "outputId": "ae56530a-8154-479c-e5dc-dd61f8dd4f3d"
      },
      "source": [
        "gae_results = []\n",
        "best_agent, best_eval_score = None, float('-inf')\n",
        "for seed in SEEDS:\n",
        "    environment_settings = {\n",
        "        'env_name': 'CartPole-v1',\n",
        "        'gamma': 0.99,\n",
        "        'max_minutes': 10,\n",
        "        'max_episodes': 10000,\n",
        "        'goal_mean_100_reward': 475\n",
        "    }\n",
        "\n",
        "    policy_model_fn = lambda nS, nA: FCDAP(nS, nA, hidden_dims=(128,64))\n",
        "    policy_model_max_grad_norm = 1\n",
        "    policy_optimizer_fn = lambda net, lr: SharedAdam(net.parameters(), lr=lr)\n",
        "    policy_optimizer_lr = 0.0005\n",
        "\n",
        "    value_model_fn = lambda nS: FCV(nS, hidden_dims=(256,128))\n",
        "    value_model_max_grad_norm = float('inf')\n",
        "    value_optimizer_fn = lambda net, lr: SharedRMSprop(net.parameters(), lr=lr)\n",
        "    value_optimizer_lr = 0.0007\n",
        "\n",
        "    entropy_loss_weight = 0.001\n",
        "\n",
        "    max_n_steps = 50\n",
        "    n_workers = 8\n",
        "    tau = 0.95\n",
        "\n",
        "    env_name, gamma, max_minutes, \\\n",
        "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
        "    agent = GAE(policy_model_fn,\n",
        "                policy_model_max_grad_norm, \n",
        "                policy_optimizer_fn, \n",
        "                policy_optimizer_lr,\n",
        "                value_model_fn,\n",
        "                value_model_max_grad_norm,\n",
        "                value_optimizer_fn, \n",
        "                value_optimizer_lr, \n",
        "                entropy_loss_weight,\n",
        "                max_n_steps,\n",
        "                n_workers,\n",
        "                tau)\n",
        "\n",
        "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n",
        "    result, final_eval_score, training_time, wallclock_time = agent.train(\n",
        "        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
        "    gae_results.append(result)\n",
        "    if final_eval_score > best_eval_score:\n",
        "        best_eval_score = final_eval_score\n",
        "        best_agent = agent\n",
        "gae_results = np.array(gae_results)\n",
        "_ = BEEP()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[2Kel 00:00:00, ep 0000, ts 000015, ar 10 015.0±000.0, 100 015.0±000.0, ex 100 0.7±0.0, ev 011.0±000.0\n",
            "\u001b[2Kel 00:01:01, ep 0409, ts 043822, ar 10 139.5±125.5, 100 220.6±104.9, ex 100 0.3±0.1, ev 370.4±124.2\n",
            "\u001b[2Kel 00:01:59, ep 0548, ts 101746, ar 10 500.0±000.0, 100 446.2±091.4, ex 100 0.2±0.0, ev 485.4±051.3\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 499.65±3.48 in 799.61s training time, 119.04s wall-clock time.\n",
            "\n",
            "\u001b[2Kel 00:00:00, ep 0000, ts 000016, ar 10 016.0±000.0, 100 016.0±000.0, ex 100 0.5±0.0, ev 011.0±000.0\n",
            "\u001b[2Kel 00:01:02, ep 0478, ts 041655, ar 10 224.7±077.4, 100 197.1±084.9, ex 100 0.3±0.0, ev 307.5±149.3\n",
            "\u001b[2Kel 00:01:44, ep 0587, ts 079503, ar 10 388.6±125.4, 100 356.8±120.4, ex 100 0.3±0.0, ev 475.9±063.6\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 441.28±61.34 in 706.53s training time, 105.61s wall-clock time.\n",
            "\n",
            "\u001b[2Kel 00:00:00, ep 0000, ts 000012, ar 10 012.0±000.0, 100 012.0±000.0, ex 100 0.2±0.0, ev 009.0±000.0\n",
            "\u001b[2Kel 00:01:02, ep 0490, ts 040760, ar 10 247.5±081.5, 100 207.7±108.6, ex 100 0.3±0.0, ev 332.7±133.1\n",
            "\u001b[2Kel 00:01:57, ep 0625, ts 092901, ar 10 449.5±095.7, 100 413.1±102.9, ex 100 0.2±0.0, ev 475.5±063.1\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 499.57±4.28 in 807.32s training time, 120.24s wall-clock time.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ud4aFWz5ON6e",
        "outputId": "ccc86f41-bcf9-42fe-c31d-fa1a49a4b4de"
      },
      "source": [
        "a2c_results = []\n",
        "best_agent, best_eval_score = None, float('-inf')\n",
        "for seed in SEEDS:\n",
        "    environment_settings = {\n",
        "        'env_name': 'CartPole-v1',\n",
        "        'gamma': 0.99,\n",
        "        'max_minutes': 10,\n",
        "        'max_episodes': 10000,\n",
        "        'goal_mean_100_reward': 475\n",
        "    }\n",
        "    \n",
        "    ac_model_fn = lambda nS, nA: FCAC(nS, nA, hidden_dims=(256,128))\n",
        "    ac_model_max_grad_norm = 1\n",
        "    # ac_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
        "    ac_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n",
        "    ac_optimizer_lr = 0.001\n",
        "\n",
        "    policy_loss_weight = 1.0\n",
        "    value_loss_weight = 0.6\n",
        "\n",
        "    entropy_loss_weight = 0.001\n",
        "\n",
        "    max_n_steps = 10\n",
        "    n_workers = 8\n",
        "    tau = 0.95\n",
        "\n",
        "    env_name, gamma, max_minutes, \\\n",
        "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
        "    agent = A2C(ac_model_fn, \n",
        "                ac_model_max_grad_norm,\n",
        "                ac_optimizer_fn,\n",
        "                ac_optimizer_lr,\n",
        "                policy_loss_weight,\n",
        "                value_loss_weight,\n",
        "                entropy_loss_weight,\n",
        "                max_n_steps,\n",
        "                n_workers,\n",
        "                tau)\n",
        "\n",
        "    make_envs_fn = lambda mef, mea, s, n: MultiprocessEnv(mef, mea, s, n) \n",
        "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n",
        "    result, final_eval_score, training_time, wallclock_time = agent.train(make_envs_fn,\n",
        "                                                                          make_env_fn,\n",
        "                                                                          make_env_kargs,\n",
        "                                                                          seed,\n",
        "                                                                          gamma,\n",
        "                                                                          max_minutes,\n",
        "                                                                          max_episodes,\n",
        "                                                                          goal_mean_100_reward)\n",
        "    a2c_results.append(result)\n",
        "    if final_eval_score > best_eval_score:\n",
        "        best_eval_score = final_eval_score\n",
        "        best_agent = agent\n",
        "a2c_results = np.array(a2c_results)\n",
        "_ = BEEP()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[2Kel 00:00:00, ep 0000, ts 000014, ar 10 014.0±000.0, 100 014.0±000.0, ex 100 0.5±0.0, ev 010.0±000.0\n",
            "\u001b[2Kel 00:01:00, ep 0702, ts 103659, ar 10 440.9±090.8, 100 324.6±161.4, ex 100 0.2±0.0, ev 443.6±088.7\n",
            "\u001b[2Kel 00:01:18, ep 0775, ts 139752, ar 10 500.0±000.0, 100 460.7±092.7, ex 100 0.2±0.0, ev 476.5±057.3\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 499.70±2.26 in 589.82s training time, 86.82s wall-clock time.\n",
            "\n",
            "\u001b[2Kel 00:00:00, ep 0000, ts 000014, ar 10 014.0±000.0, 100 014.0±000.0, ex 100 0.4±0.0, ev 011.0±000.0\n",
            "\u001b[2Kel 00:01:00, ep 0703, ts 107602, ar 10 471.7±084.9, 100 407.4±141.2, ex 100 0.2±0.0, ev 470.0±057.0\n",
            "\u001b[2Kel 00:01:03, ep 0714, ts 112568, ar 10 466.1±101.7, 100 423.2±135.8, ex 100 0.2±0.0, ev 476.7±050.8\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 500.00±0.00 in 473.37s training time, 71.51s wall-clock time.\n",
            "\n",
            "\u001b[2Kel 00:00:00, ep 0000, ts 000009, ar 10 009.0±000.0, 100 009.0±000.0, ex 100 0.9±0.0, ev 010.0±000.0\n",
            "\u001b[2Kel 00:00:53, ep 0648, ts 096132, ar 10 455.8±132.6, 100 384.5±157.4, ex 100 0.2±0.0, ev 475.7±062.1\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 500.00±0.00 in 399.11s training time, 61.34s wall-clock time.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGG3fVvkOQEF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}