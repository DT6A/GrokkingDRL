{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "from itertools import cycle, count\n",
    "from textwrap import wrap\n",
    "\n",
    "import matplotlib\n",
    "import subprocess\n",
    "import os.path\n",
    "import tempfile\n",
    "import random\n",
    "import base64\n",
    "import pprint\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "import gym\n",
    "import io\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from gym import wrappers\n",
    "from subprocess import check_output\n",
    "from IPython.display import HTML\n",
    "\n",
    "from zoo.actor_critic_agents import *\n",
    "from zoo.advanced_actor_critic_agents import *\n",
    "from zoo.exploration_strategies import *\n",
    "from zoo.replay_buffers import *\n",
    "from zoo.utils import *\n",
    "\n",
    "SEEDS = (12, 34, 56)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2Kel 00:00:00, ep 0000, ts 0000200, ar 10 -1296.7±000.0, 100 -1296.7±000.0, ex 100 0.3±0.0, ev -1391.7±000.0\n",
      "\u001b[2Kel 00:01:01, ep 0029, ts 0006000, ar 10 -163.7±096.1, 100 -798.0±631.2, ex 100 0.1±0.1, ev -785.3±601.3\n",
      "\u001b[2Kel 00:02:02, ep 0053, ts 0010800, ar 10 -161.0±053.1, 100 -510.1±571.4, ex 100 0.1±0.1, ev -491.8±557.3\n",
      "\u001b[2Kel 00:03:03, ep 0076, ts 0015400, ar 10 -168.5±090.7, 100 -407.9±506.2, ex 100 0.1±0.1, ev -390.7±494.0\n",
      "\u001b[2Kel 00:04:05, ep 0099, ts 0020000, ar 10 -156.2±086.5, 100 -349.7±458.5, ex 100 0.1±0.1, ev -330.6±449.5\n",
      "\u001b[2Kel 00:04:51, ep 0116, ts 0023400, ar 10 -144.7±069.7, 100 -176.4±174.4, ex 100 0.0±0.0, ev -146.3±097.0\n",
      "--> reached_goal_mean_reward ✓\n",
      "Training complete.\n",
      "Final evaluation score -125.91±78.89 in 274.11s training time, 310.43s wall-clock time.\n",
      "\n",
      "\u001b[2Kel 00:00:00, ep 0000, ts 0000200, ar 10 -1373.7±000.0, 100 -1373.7±000.0, ex 100 0.3±0.0, ev -1743.3±000.0\n",
      "\u001b[2Kel 00:01:01, ep 0026, ts 0005400, ar 10 -260.7±252.5, 100 -906.3±538.2, ex 100 0.1±0.1, ev -1005.7±570.3\n",
      "\u001b[2Kel 00:02:03, ep 0048, ts 0009800, ar 10 -111.8±086.7, 100 -559.4±557.2, ex 100 0.1±0.1, ev -628.6±597.5\n",
      "\u001b[2Kel 00:03:04, ep 0071, ts 0014400, ar 10 -142.0±095.4, 100 -430.1±500.6, ex 100 0.1±0.1, ev -479.0±541.1\n",
      "\u001b[2Kel 00:04:06, ep 0093, ts 0018800, ar 10 -143.5±049.2, 100 -358.8±457.9, ex 100 0.1±0.1, ev -395.1±499.1\n",
      "\u001b[2Kel 00:05:02, ep 0110, ts 0022200, ar 10 -108.3±036.2, 100 -215.7±295.2, ex 100 0.0±0.0, ev -254.4±345.9\n",
      "--> reached_max_minutes ✕\n",
      "Training complete.\n",
      "Final evaluation score -145.81±81.53 in 283.68s training time, 319.56s wall-clock time.\n",
      "\n",
      "\u001b[2Kel 00:00:00, ep 0000, ts 0000200, ar 10 -1364.0±000.0, 100 -1364.0±000.0, ex 100 0.3±0.0, ev -1923.0±000.0\n",
      "\u001b[2Kel 00:01:03, ep 0021, ts 0004400, ar 10 -1123.0±573.1, 100 -1208.6±500.4, ex 100 0.1±0.1, ev -1305.9±546.8\n",
      "\u001b[2Kel 00:02:05, ep 0041, ts 0008400, ar 10 -165.2±059.6, 100 -870.3±639.8, ex 100 0.1±0.1, ev -930.2±698.4\n",
      "\u001b[2Kel 00:03:08, ep 0064, ts 0013000, ar 10 -183.6±061.3, 100 -617.1±618.8, ex 100 0.1±0.1, ev -650.3±678.3\n",
      "\u001b[2Kel 00:04:09, ep 0086, ts 0017400, ar 10 -156.4±075.6, 100 -504.0±570.9, ex 100 0.0±0.1, ev -533.9±621.0\n",
      "\u001b[2Kel 00:05:02, ep 0105, ts 0021200, ar 10 -189.2±129.0, 100 -392.7±499.6, ex 100 0.0±0.0, ev -395.9±515.0\n",
      "--> reached_max_minutes ✕\n",
      "Training complete.\n",
      "Final evaluation score -157.04±90.86 in 283.22s training time, 317.58s wall-clock time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddpg_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "for seed in SEEDS:\n",
    "    environment_settings = {\n",
    "        'env_name': 'Pendulum-v0',\n",
    "        'gamma': 0.99,\n",
    "        'max_minutes': 5,\n",
    "        'max_episodes': 500,\n",
    "        'goal_mean_100_reward': -150\n",
    "    }\n",
    "\n",
    "    policy_model_fn = lambda nS, bounds: FCDP(nS, bounds, hidden_dims=(256,256))\n",
    "    policy_max_grad_norm = float('inf')\n",
    "    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    policy_optimizer_lr = 0.0003\n",
    "\n",
    "    value_model_fn = lambda nS, nA: FCQV(nS, nA, hidden_dims=(256,256))\n",
    "    value_max_grad_norm = float('inf')\n",
    "    value_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    value_optimizer_lr = 0.0003\n",
    "\n",
    "    training_strategy_fn = lambda bounds: NormalNoiseStrategy(bounds, exploration_noise_ratio=0.1)\n",
    "    evaluation_strategy_fn = lambda bounds: GreedyStrategyC(bounds)\n",
    "\n",
    "    replay_buffer_fn = lambda: ReplayBuffer(m_size=100000, batch_size=256)\n",
    "    n_warmup_batches = 5\n",
    "    update_target_every_steps = 1\n",
    "    tau = 0.005\n",
    "    \n",
    "    env_name, gamma, max_minutes, \\\n",
    "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
    "\n",
    "    agent = DDPG(replay_buffer_fn,\n",
    "                 policy_model_fn, \n",
    "                 policy_max_grad_norm, \n",
    "                 policy_optimizer_fn, \n",
    "                 policy_optimizer_lr,\n",
    "                 value_model_fn, \n",
    "                 value_max_grad_norm, \n",
    "                 value_optimizer_fn, \n",
    "                 value_optimizer_lr, \n",
    "                 training_strategy_fn,\n",
    "                 evaluation_strategy_fn,\n",
    "                 n_warmup_batches,\n",
    "                 update_target_every_steps,\n",
    "                 tau)\n",
    "\n",
    "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n",
    "    result, final_eval_score, training_time, wallclock_time = agent.train(\n",
    "        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
    "    ddpg_results.append(result)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_agent = agent\n",
    "ddpg_results = np.array(ddpg_results)\n",
    "_ = BEEP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenderUint8(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "    def render(self, mode='rgb_array'):\n",
    "        frame = self.env.render(mode=mode)\n",
    "        return frame.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2Kel 00:00:01, ep 0000, ts 0000200, ar 10 -1194.6±000.0, 100 -1194.6±000.0, ex 100 0.3±0.0, ev -884.9±000.0\n",
      "\u001b[2Kel 00:01:02, ep 0030, ts 0006200, ar 10 -980.7±229.8, 100 -1255.5±312.9, ex 100 0.2±0.1, ev -1217.9±363.7\n",
      "\u001b[2Kel 00:02:03, ep 0055, ts 0011200, ar 10 -182.8±109.9, 100 -797.2±570.7, ex 100 0.2±0.0, ev -736.8±602.6\n",
      "\u001b[2Kel 00:03:04, ep 0078, ts 0015800, ar 10 -171.2±096.6, 100 -621.2±556.1, ex 100 0.2±0.0, ev -560.4±578.6\n",
      "\u001b[2Kel 00:04:06, ep 0102, ts 0020600, ar 10 -199.9±086.3, 100 -503.8±520.1, ex 100 0.2±0.0, ev -443.9±533.1\n",
      "\u001b[2Kel 00:05:01, ep 0122, ts 0024600, ar 10 -277.9±089.1, 100 -269.4±237.5, ex 100 0.1±0.0, ev -192.0±212.5\n",
      "--> reached_max_minutes ✕\n",
      "Training complete.\n",
      "Final evaluation score -138.83±81.93 in 281.81s training time, 316.41s wall-clock time.\n",
      "\n",
      "\u001b[2Kel 00:00:00, ep 0000, ts 0000200, ar 10 -1350.2±000.0, 100 -1350.2±000.0, ex 100 0.3±0.0, ev -1885.2±000.0\n",
      "\u001b[2Kel 00:01:00, ep 0028, ts 0005800, ar 10 -889.5±351.3, 100 -1264.8±370.6, ex 100 0.2±0.1, ev -1296.4±347.9\n",
      "\u001b[2Kel 00:02:02, ep 0052, ts 0010600, ar 10 -184.6±123.1, 100 -807.5±589.7, ex 100 0.2±0.1, ev -813.6±610.9\n",
      "\u001b[2Kel 00:03:04, ep 0075, ts 0015200, ar 10 -173.4±115.2, 100 -607.0±582.5, ex 100 0.2±0.0, ev -611.5±596.8\n",
      "\u001b[2Kel 00:04:04, ep 0097, ts 0019600, ar 10 -244.5±099.3, 100 -514.0±543.4, ex 100 0.2±0.0, ev -504.4±563.8\n",
      "\u001b[2Kel 00:05:02, ep 0118, ts 0023800, ar 10 -254.8±107.4, 100 -272.7±272.9, ex 100 0.1±0.0, ev -242.0±292.4\n",
      "--> reached_max_minutes ✕\n",
      "Training complete.\n",
      "Final evaluation score -146.20±75.16 in 282.73s training time, 317.96s wall-clock time.\n",
      "\n",
      "\u001b[2Kel 00:00:00, ep 0000, ts 0000200, ar 10 -1304.1±000.0, 100 -1304.1±000.0, ex 100 0.3±0.0, ev -1885.3±000.0\n",
      "\u001b[2Kel 00:01:02, ep 0029, ts 0006000, ar 10 -722.3±352.5, 100 -1183.1±428.9, ex 100 0.2±0.1, ev -1109.2±590.2\n",
      "\u001b[2Kel 00:02:03, ep 0051, ts 0010400, ar 10 -161.0±077.0, 100 -748.9±604.8, ex 100 0.2±0.1, ev -721.4±652.5\n",
      "\u001b[2Kel 00:03:05, ep 0074, ts 0015000, ar 10 -163.3±086.3, 100 -596.6±569.3, ex 100 0.2±0.0, ev -616.3±612.7\n",
      "\u001b[2Kel 00:04:06, ep 0097, ts 0019600, ar 10 -182.5±103.9, 100 -501.0±529.4, ex 100 0.2±0.0, ev -514.2±568.0\n",
      "\u001b[2Kel 00:05:00, ep 0117, ts 0023600, ar 10 -157.0±110.9, 100 -269.6±280.7, ex 100 0.2±0.0, ev -262.7±311.5\n",
      "--> reached_max_minutes ✕\n",
      "Training complete.\n",
      "Final evaluation score -156.00±90.86 in 280.29s training time, 315.87s wall-clock time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "td3_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "for seed in SEEDS:\n",
    "    environment_settings = {\n",
    "        'env_name': 'Pendulum-v0',\n",
    "        'gamma': 0.99,\n",
    "        'max_minutes': 5,\n",
    "        'max_episodes': 500,\n",
    "        'goal_mean_100_reward': -150\n",
    "    }\n",
    "    \n",
    "    policy_model_fn = lambda nS, bounds: FCDP(nS, bounds, hidden_dims=(256,256))\n",
    "    policy_max_grad_norm = float('inf')\n",
    "    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    policy_optimizer_lr = 0.0003\n",
    "\n",
    "    value_model_fn = lambda nS, nA: FCTQV(nS, nA, hidden_dims=(256,256))\n",
    "    value_max_grad_norm = float('inf')\n",
    "    value_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    value_optimizer_lr = 0.0003\n",
    "\n",
    "    training_strategy_fn = lambda bounds: NormalNoiseDecayStrategy(bounds,\n",
    "                                                                   init_noise_ratio=0.5,\n",
    "                                                                   min_noise_ratio=0.1,\n",
    "                                                                   decay_steps=200000)\n",
    "    evaluation_strategy_fn = lambda bounds: GreedyStrategyC(bounds)\n",
    "\n",
    "    replay_buffer_fn = lambda: ReplayBuffer(m_size=1000000, batch_size=256)\n",
    "    n_warmup_batches = 5\n",
    "    update_value_target_every_steps = 2\n",
    "    update_policy_target_every_steps = 2\n",
    "    train_policy_every_steps = 2\n",
    "    policy_noise_ratio = 0.1\n",
    "    policy_noise_clip_ratio = 0.5\n",
    "    tau = 0.005\n",
    "\n",
    "    env_name, gamma, max_minutes, \\\n",
    "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
    "\n",
    "    agent = TD3(replay_buffer_fn,\n",
    "                policy_model_fn, \n",
    "                policy_max_grad_norm, \n",
    "                policy_optimizer_fn, \n",
    "                policy_optimizer_lr,\n",
    "                value_model_fn,\n",
    "                value_max_grad_norm, \n",
    "                value_optimizer_fn, \n",
    "                value_optimizer_lr, \n",
    "                training_strategy_fn,\n",
    "                evaluation_strategy_fn,\n",
    "                n_warmup_batches,\n",
    "                update_value_target_every_steps,\n",
    "                update_policy_target_every_steps,\n",
    "                train_policy_every_steps,\n",
    "                tau,\n",
    "                policy_noise_ratio,\n",
    "                policy_noise_clip_ratio)\n",
    "\n",
    "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n",
    "    result, final_eval_score, training_time, wallclock_time = agent.train(\n",
    "        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
    "    td3_results.append(result)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_agent = agent\n",
    "td3_results = np.array(td3_results)\n",
    "_ = BEEP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2Kel 00:00:02, ep 0000, ts 0000200, ar 10 -904.2±000.0, 100 -904.2±000.0, ex 100 0.3±0.0, ev -1342.1±000.0\n",
      "\u001b[2Kel 00:01:06, ep 0024, ts 0005000, ar 10 -1176.2±323.1, 100 -1246.6±348.4, ex 100 0.2±0.1, ev -1273.5±471.8\n",
      "\u001b[2Kel 00:02:11, ep 0039, ts 0008000, ar 10 -124.1±053.0, 100 -835.6±601.3, ex 100 0.2±0.1, ev -851.8±661.8\n",
      "\u001b[2Kel 00:03:12, ep 0052, ts 0010600, ar 10 -167.0±081.3, 100 -673.3±596.1, ex 100 0.2±0.1, ev -671.7±657.0\n",
      "\u001b[2Kel 00:04:14, ep 0066, ts 0013400, ar 10 -193.2±078.4, 100 -565.0±572.1, ex 100 0.2±0.1, ev -562.0±622.9\n",
      "\u001b[2Kel 00:05:03, ep 0077, ts 0015600, ar 10 -145.4±071.3, 100 -508.1±549.5, ex 100 0.2±0.1, ev -501.3±597.5\n",
      "--> reached_max_minutes ✕\n",
      "Training complete.\n",
      "Final evaluation score -136.60±82.74 in 275.30s training time, 337.30s wall-clock time.\n",
      "\n",
      "\u001b[2Kel 00:00:00, ep 0000, ts 0000200, ar 10 -1242.3±000.0, 100 -1242.3±000.0, ex 100 0.3±0.0, ev -1877.7±000.0\n",
      "\u001b[2Kel 00:01:01, ep 0023, ts 0004800, ar 10 -1251.1±294.2, 100 -1318.8±285.2, ex 100 0.2±0.1, ev -1285.5±373.3\n",
      "\u001b[2Kel 00:02:04, ep 0037, ts 0007600, ar 10 -153.6±079.8, 100 -902.7±593.7, ex 100 0.2±0.1, ev -877.5±613.7\n",
      "\u001b[2Kel 00:03:05, ep 0051, ts 0010400, ar 10 -122.2±078.5, 100 -692.2±616.1, ex 100 0.2±0.1, ev -693.5±607.9\n",
      "\u001b[2Kel 00:04:07, ep 0065, ts 0013200, ar 10 -109.6±084.3, 100 -570.8±596.5, ex 100 0.2±0.1, ev -581.1±582.7\n",
      "\u001b[2Kel 00:05:00, ep 0077, ts 0015600, ar 10 -141.5±068.4, 100 -507.3±569.3, ex 100 0.2±0.1, ev -509.9±562.1\n",
      "--> reached_max_minutes ✕\n",
      "Training complete.\n",
      "Final evaluation score -129.06±76.88 in 274.57s training time, 332.66s wall-clock time.\n",
      "\n",
      "\u001b[2Kel 00:00:00, ep 0000, ts 0000200, ar 10 -852.3±000.0, 100 -852.3±000.0, ex 100 0.3±0.0, ev -1826.8±000.0\n",
      "\u001b[2Kel 00:01:01, ep 0023, ts 0004800, ar 10 -1300.8±289.1, 100 -1180.7±313.6, ex 100 0.2±0.1, ev -1313.6±345.6\n",
      "\u001b[2Kel 00:02:03, ep 0037, ts 0007600, ar 10 -163.9±081.4, 100 -813.3±550.1, ex 100 0.2±0.1, ev -869.9±644.5\n",
      "\u001b[2Kel 00:03:04, ep 0051, ts 0010400, ar 10 -143.5±043.9, 100 -631.1±558.3, ex 100 0.2±0.1, ev -679.0±636.3\n",
      "\u001b[2Kel 00:04:06, ep 0065, ts 0013200, ar 10 -199.5±071.3, 100 -532.8±531.9, ex 100 0.2±0.1, ev -563.7±607.5\n",
      "\u001b[2Kel 00:05:04, ep 0078, ts 0015800, ar 10 -172.0±112.3, 100 -472.8±506.3, ex 100 0.2±0.0, ev -502.5±573.5\n",
      "--> reached_max_minutes ✕\n",
      "Training complete.\n",
      "Final evaluation score -155.42±80.04 in 278.15s training time, 336.33s wall-clock time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sac_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "for seed in SEEDS:\n",
    "    environment_settings = {\n",
    "        'env_name': 'Pendulum-v0',\n",
    "        'gamma': 0.99,\n",
    "        'max_minutes': 5,\n",
    "        'max_episodes': 500,\n",
    "        'goal_mean_100_reward': -150\n",
    "    }\n",
    "\n",
    "    policy_model_fn = lambda nS, bounds: FCGP(nS, bounds, hidden_dims=(256,256))\n",
    "    policy_max_grad_norm = float('inf')\n",
    "    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    policy_optimizer_lr = 0.0005\n",
    "\n",
    "    value_model_fn = lambda nS, nA: FCQSA(nS, nA, hidden_dims=(256,256))\n",
    "    value_max_grad_norm = float('inf')\n",
    "    value_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    value_optimizer_lr = 0.0007\n",
    "\n",
    "    replay_buffer_fn = lambda: ReplayBuffer(m_size=1000000, batch_size=256)\n",
    "    n_warmup_batches = 10\n",
    "    update_target_every_steps = 1\n",
    "    tau = 0.005\n",
    "\n",
    "    env_name, gamma, max_minutes, \\\n",
    "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
    "                \n",
    "    agent = SAC(replay_buffer_fn,\n",
    "                policy_model_fn, \n",
    "                policy_max_grad_norm,\n",
    "                policy_optimizer_fn, \n",
    "                policy_optimizer_lr,\n",
    "                value_model_fn,\n",
    "                value_max_grad_norm, \n",
    "                value_optimizer_fn, \n",
    "                value_optimizer_lr, \n",
    "                n_warmup_batches,\n",
    "                update_target_every_steps,\n",
    "                tau)\n",
    "\n",
    "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n",
    "    result, final_eval_score, training_time, wallclock_time = agent.train(\n",
    "        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
    "    sac_results.append(result)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_agent = agent\n",
    "sac_results = np.array(sac_results)\n",
    "_ = BEEP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
